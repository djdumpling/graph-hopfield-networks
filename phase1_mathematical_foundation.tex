\documentclass[11pt]{exam}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[retro,cabin,sleek]{dylanadi}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\DeclareMathOperator{\diag}{diag}

\title{Graph Hopfield Networks: Mathematical Foundation}
\author{Phase 1: Complete Mathematical Derivation}
\date{}

\DeclareMathOperator{\lse}{lse}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\softmax}{softmax}

\begin{document}

\maketitle

\section{Introduction}

This document provides the complete mathematical foundation for Graph Hopfield Networks (GHN), extending Modern Continuous Hopfield Networks to graph-structured data. The goal is to derive a rigorous energy function, update dynamics, and prove convergence guarantees.

\section{Background: Modern Continuous Hopfield Networks}

\subsection{Classical Formulation}

For a query vector $\xi \in \mathbb{R}^d$ and a memory bank $M = [m_1, m_2, \ldots, m_N]^T \in \mathbb{R}^{N \times d}$ containing $N$ stored patterns, the Modern Hopfield Network (MCHN) energy function is:

\begin{equation}
E_{\text{MH}}(\xi) = -\beta^{-1} \log \sum_{i=1}^{N} \exp(\beta \cdot \xi^T m_i) + \frac{1}{2}\|\xi\|^2
\label{eq:mhn_energy}
\end{equation}

where $\beta > 0$ is the inverse temperature parameter controlling pattern selectivity.

\subsection{Update Rule}

The fixed-point update rule for MCHN is:

\begin{equation}
\xi_{\text{new}} = M^T \cdot \softmax(\beta \cdot M \xi) = \sum_{i=1}^{N} \frac{\exp(\beta \cdot \xi^T m_i)}{\sum_{j=1}^{N} \exp(\beta \cdot \xi^T m_j)} m_i
\label{eq:mhn_update}
\end{equation}

This can be interpreted as attention-weighted pattern retrieval, where patterns similar to the query receive higher weights.

\subsection{Log-Sum-Exp Function}

The log-sum-exp function is defined as:
\begin{equation}
\lse(\beta, \mathbf{z}) = \beta^{-1} \log \sum_{i=1}^{N} \exp(\beta z_i)
\end{equation}

where $\mathbf{z} = [z_1, \ldots, z_N]^T$. This function is convex and smooth, with gradient:
\begin{equation}
\nabla_{\mathbf{z}} \lse(\beta, \mathbf{z}) = \softmax(\beta \mathbf{z})
\end{equation}

\section{Graph Hopfield Networks: Energy Function}

\subsection{Problem Setup}

Consider a graph $G = (V, E)$ with:
\begin{itemize}
    \item $|V| = n$ nodes
    \item Edge set $E \subseteq V \times V$
    \item Node feature matrix $X = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^{n \times d}$ where $x_v \in \mathbb{R}^d$ is the feature vector for node $v$
    \item Memory bank $M = [m_1, m_2, \ldots, m_N]^T \in \mathbb{R}^{N \times d}$ containing $N$ stored patterns
    \item Adjacency matrix $A \in \{0,1\}^{n \times n}$ where $A_{uv} = 1$ if $(u,v) \in E$
    \item Degree matrix $D = \text{diag}(d_1, \ldots, d_n)$ where $d_v = \sum_{u} A_{uv}$
    \item Graph Laplacian $L = D - A$ (unnormalized) or $L = I - D^{-1/2} A D^{-1/2}$ (normalized)
\end{itemize}

\textbf{Graph convention:} Throughout this document, we assume:
\begin{enumerate}
    \item The graph is \textbf{undirected}, i.e., the adjacency matrix $A$ is symmetric: $A_{uv} = A_{vu}$ for all $u,v$.
    \item Each undirected edge is \textbf{counted exactly once} in the edge set $E$. Formally, if $(u,v) \in E$ with $u \neq v$, then $(v,u) \notin E$ (we do not double-count).
    \item The Laplacian $L = D - A$ is therefore symmetric and positive semi-definite.
\end{enumerate}
These conventions are essential for the Laplacian identity $\sum_{(u,v) \in E} \|x_u - x_v\|^2 = \tr(X^T L X)$ and the gradient computations to hold with the stated constants.

\subsection{Graph Hopfield Energy Function}

The Graph Hopfield energy function combines associative memory retrieval with graph structure regularization:

\begin{equation}
E_{\text{GH}}(X) = \sum_{v \in V} \left[ -\lse(\beta, M x_v) + \frac{1}{2}\|x_v\|^2 \right] + \lambda \sum_{\{u,v\} \in E} \|x_u - x_v\|^2
\label{eq:gh_energy_node}
\end{equation}

where:
\begin{itemize}
    \item First term: Hopfield retrieval energy per node (attracts node embeddings toward stored patterns)
    \item Second term: Graph Laplacian regularization (enforces smoothness between neighboring nodes)
    \item $\beta > 0$: Inverse temperature (controls pattern selectivity)
    \item $\lambda \geq 0$: Graph smoothness weight
\end{itemize}

\subsection{Matrix Formulation}

The energy can be written in compact matrix form:

\begin{equation}
E_{\text{GH}}(X) = -\sum_{v=1}^{n} \lse(\beta, M x_v) + \frac{1}{2}\|X\|_F^2 + \lambda \cdot \tr(X^T L X)
\label{eq:gh_energy_matrix}
\end{equation}

where $\|X\|_F^2 = \sum_{v=1}^{n} \|x_v\|^2$ is the Frobenius norm squared, and $L$ is the graph Laplacian.

\textbf{Derivation of Laplacian term (using the count-once convention):}

Under our undirected graph convention (each edge counted exactly once in $E$ as an unordered pair, with no self-loops: $A_{vv} = 0$ for all $v$), we have:
\begin{align}
\sum_{\{u,v\} \in E} \|x_u - x_v\|^2 &= \sum_{\{u,v\} \in E} (x_u - x_v)^T (x_u - x_v) \\
&= \sum_{\{u,v\} \in E} (x_u^T x_u + x_v^T x_v - 2 x_u^T x_v) \\
&= \sum_{v} d_v x_v^T x_v - 2 \sum_{\{u,v\} \in E} x_u^T x_v \\
&= \tr(X^T D X) - \tr(X^T A X) \\
&= \tr(X^T (D - A) X) = \tr(X^T L X)
\end{align}
where line 3 uses: (i) each node $v$ appears in exactly $d_v$ edges, contributing $d_v x_v^T x_v$, and (ii) since $A$ is symmetric and $A_{uv} = 1$ iff $\{u,v\} \in E$ (with $A_{vv} = 0$), we have $\tr(X^T A X) = \sum_{u,v} A_{uv} x_u^T x_v = 2\sum_{\{u,v\} \in E} x_u^T x_v$ (the factor of 2 accounts for the fact that $A_{uv} = A_{vu} = 1$ for each edge $\{u,v\}$). The diagonal terms $A_{vv} = 0$ ensure no self-loop contributions.

\section{Update Dynamics}

\subsection{Gradient Computation}

To derive the update dynamics, we compute the gradient of the energy with respect to each node's feature vector.

\textbf{For node $v$:}

\begin{align}
\nabla_{x_v} E_{\text{GH}}(X) &= \nabla_{x_v} \left[ -\lse(\beta, M x_v) + \frac{1}{2}\|x_v\|^2 + \lambda \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2 \right] \\
&= -\nabla_{x_v} \lse(\beta, M x_v) + x_v + \lambda \nabla_{x_v} \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2
\end{align}

\textbf{First term (Hopfield retrieval):}

Let $\mathbf{z} = M x_v \in \mathbb{R}^N$ where $z_i = m_i^T x_v$. By the chain rule:
\begin{align}
\nabla_{x_v} \lse(\beta, M x_v) &= M^T \nabla_{\mathbf{z}} \lse(\beta, \mathbf{z}) \\
&= M^T \softmax(\beta \mathbf{z}) \\
&= M^T \softmax(\beta M x_v)
\end{align}
where we used $\nabla_{\mathbf{z}} \lse(\beta, \mathbf{z}) = \softmax(\beta \mathbf{z})$ from Section 2.3.

\textbf{Third term (Graph regularization):}
\begin{align}
\nabla_{x_v} \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2 &= \sum_{u \in \mathcal{N}(v)} 2(x_v - x_u) \\
&= 2d_v x_v - 2 \sum_{u \in \mathcal{N}(v)} x_u \\
&= 2(L X)_v
\end{align}

where $(L X)_v$ denotes the $v$-th row of $L X$.

\textbf{Complete gradient:}
\begin{equation}
\nabla_{x_v} E_{\text{GH}}(X) = -M^T \softmax(\beta \cdot M x_v) + x_v + 2\lambda (L X)_v
\label{eq:gradient}
\end{equation}

\subsection{Fixed-Point Iteration}

Using gradient descent with step size $\eta > 0$:

\begin{equation}
x_v^{(t+1)} = x_v^{(t)} - \eta \nabla_{x_v} E_{\text{GH}}(X^{(t)})
\end{equation}

Substituting the gradient:

\begin{equation}
x_v^{(t+1)} = x_v^{(t)} + \eta \left[ M^T \softmax(\beta \cdot M x_v^{(t)}) - x_v^{(t)} - 2\lambda (L X^{(t)})_v \right]
\label{eq:update_gd}
\end{equation}

\subsection{Alternative: Direct Fixed-Point Update}

Setting the gradient to zero gives the fixed-point condition:

\begin{equation}
x_v^* = M^T \softmax(\beta \cdot M x_v^*) - 2\lambda (L X^*)_v
\label{eq:fixed_point}
\end{equation}

This suggests an iterative update rule:

\begin{equation}
x_v^{(t+1)} = M^T \softmax(\beta \cdot M x_v^{(t)}) - 2\lambda (L X^{(t)})_v
\label{eq:update_fixed}
\end{equation}

\subsection{With Damping/Momentum}

For numerical stability, we can add damping:

\begin{equation}
x_v^{(t+1)} = (1-\alpha) x_v^{(t)} + \alpha \left[ M^T \softmax(\beta \cdot M x_v^{(t)}) - 2\lambda (L X^{(t)})_v \right]
\label{eq:update_damped}
\end{equation}

where $\alpha \in (0,1]$ is the damping coefficient.

\section{Convergence Analysis}

\subsection{Convexity Properties}

\begin{lemma}[Convexity of Log-Sum-Exp]
The function $f(x) = \lse(\beta, M x)$ is convex in $x$.
\end{lemma}

\begin{proof}
The log-sum-exp function $h(z) = \beta^{-1} \log \sum_i \exp(\beta z_i)$ is convex in $z$ (this is a standard result: the log of a sum of exponentials is convex). The function $f(x) = \lse(\beta, M x)$ is the composition of:
\begin{itemize}
    \item Affine function: $g(x) = M x$
    \item Convex function: $h(z) = \beta^{-1} \log \sum_i \exp(\beta z_i)$
\end{itemize}
Since the composition of a convex function with an affine function is convex, $f(x)$ is convex.
\end{proof}

\begin{lemma}[Convexity of Quadratic Regularization]
The function $f(X) = \frac{1}{2}\|X\|_F^2$ is strictly convex.
\end{lemma}

\begin{proof}
The Hessian is $\nabla^2 f(X) = I$, which is positive definite. Therefore, $f$ is strictly convex.
\end{proof}

\begin{lemma}[Convexity of Laplacian Term]
The function $f(X) = \tr(X^T L X)$ is convex (but not strictly convex if $L$ is singular).
\end{lemma}

\begin{proof}
The graph Laplacian $L$ is symmetric and positive semi-definite (all eigenvalues $\geq 0$). 
By PSD, for any $X, Y$:
\[
0 \le \tr((X-Y)^T L (X-Y)) = f(X) + f(Y) - 2\tr(X^T L Y),
\]
which implies
\[
\tr(X^T L Y) \le \tfrac{1}{2}\bigl(f(X) + f(Y)\bigr).
\]
(Note: Since $L$ is symmetric, we have $\tr(X^T L Y) = \tr(Y^T L X)$, which is used implicitly in the expansion below.)
Now expand $f(\theta X + (1-\theta)Y)$ for $\theta \in [0,1]$:
\begin{align}
f(\theta X + (1-\theta)Y) &= \theta^2 f(X) + (1-\theta)^2 f(Y) + 2\theta(1-\theta)\tr(X^T L Y) \\
&\le \theta^2 f(X) + (1-\theta)^2 f(Y) + \theta(1-\theta)\bigl(f(X) + f(Y)\bigr) \\
&= \theta f(X) + (1-\theta) f(Y).
\end{align}
Therefore, $f$ is convex.
\end{proof}

\begin{prop}[Convexity, strong convexity, and DC structure]
\label{prop:convex_dc}
Let $E_{\text{GH}}(X)$ be the Graph Hopfield energy
\[
E_{\text{GH}}(X) \;=\; \sum_{v \in V} \left[ -\lse(\beta, M x_v) + \frac{1}{2}\|x_v\|^2 \right]
\;+\;\lambda \sum_{\{u,v\} \in E} \|x_u - x_v\|^2,
\]
and let $L_{\text{lip}}$ denote any global Lipschitz constant of $\nabla E_{\text{GH}}$ (e.g. Proposition~\ref{prop:lipschitz}).

\begin{enumerate}
\item \textbf{(Generally nonconvex)} In general $E_{\text{GH}}$ need not be convex because the term
$-\lse(\beta, M x_v)$ is concave in $x_v$ (as the negation of a convex function).

\item \textbf{(Sufficient condition for convexity / strong convexity)}  
Define $p_v(X):=\softmax(\beta M x_v)\in\mathbb{R}^N$ and $\Sigma(p):=\diag(p)-pp^\top$.
For each node $v$, the Hessian of the Hopfield retrieval term is:
\[
\nabla_{x_v}^2 \lse(\beta, M x_v) = \beta M^\top \Sigma(p_v(X)) M.
\]
The covariance matrix $\Sigma(p)$ is positive semidefinite (it is the covariance matrix of a categorical distribution) and satisfies the tighter bound $\Sigma(p) \preceq \frac{1}{2}I$. (Sketch: for any unit vector $u$, $u^\top\Sigma(p)u=\mathrm{Var}(u_I)$ with $I\sim p$, and by Popoviciu's inequality $\mathrm{Var}(u_I)\le (\max_i u_i-\min_i u_i)^2/4\le 1/2$.) Therefore,
\[
0 \preceq \Sigma(p_v(X)) \preceq \tfrac{1}{2}I
\quad\Longrightarrow\quad
0 \preceq M^\top \Sigma(p_v(X)) M \preceq \tfrac{1}{2}\|M\|^2 I.
\]
Consequently, the node-wise contribution to the Hessian satisfies:
\[
\nabla_{x_v}^2 \left[ -\lse(\beta, M x_v) + \frac{1}{2}\|x_v\|^2 \right] = I - \beta M^\top \Sigma(p_v(X)) M \succeq \left(1 - \frac{\beta\|M\|^2}{2}\right) I.
\]
The Laplacian term contributes $2\lambda(L \otimes I_d)$ globally. Therefore, the (block) Hessian of $E_{\text{GH}}$ admits the uniform lower bound
\[
\nabla^2 E_{\text{GH}}(X) \;\succeq\; \bigl(1-\tfrac{\beta\|M\|^2}{2}\bigr)\,I \;+\; 2\lambda\,(L\otimes I_d),
\]
hence:
\begin{itemize}
\item if $\beta\|M\|^2 \le 2$, then $E_{\text{GH}}$ is convex;
\item if $\beta\|M\|^2 < 2$, then $E_{\text{GH}}$ is $\mu$-strongly convex with $\mu := 1-\tfrac{\beta\|M\|^2}{2}$
(and in particular has a unique global minimizer).
\end{itemize}

\textbf{Note on the Laplacian term:} The Laplacian $L$ is positive semi-definite but has a nontrivial nullspace (constant signals across all nodes). However, the global strong convexity of $E_{\text{GH}}$ comes from the $+I$ term (the $\frac{1}{2}\|X\|_F^2$ quadratic regularizer), which dominates the negative curvature from the Hopfield term when $\beta\|M\|^2 < 2$ (equivalently, $1-\tfrac{\beta\|M\|^2}{2}>0$). The Laplacian term $2\lambda(L \otimes I_d)$ adds additional positive curvature in directions orthogonal to the nullspace.

\item \textbf{(Difference-of-convex decomposition always exists)}  
Because $\nabla E_{\text{GH}}$ is $L_{\text{lip}}$-Lipschitz, we have $\nabla^2 E_{\text{GH}}(X) \preceq L_{\text{lip}} I$ for all $X$ (by the mean value theorem). This implies that the function
\[
H(X) := \frac{L_{\text{lip}}}{2}\|X\|_F^2 - E_{\text{GH}}(X)
\]
has Hessian $\nabla^2 H(X) = L_{\text{lip}} I - \nabla^2 E_{\text{GH}}(X) \succeq 0$, hence $H$ is convex.
Therefore,
\[
E_{\text{GH}}(X) \;=\; \underbrace{\frac{L_{\text{lip}}}{2}\|X\|_F^2}_{\text{convex}}
\;-\;
\underbrace{H(X)}_{\text{convex}}
\]
is a valid DC decomposition for any choice of global $L_{\text{lip}}$.
\end{enumerate}
\end{prop}

\begin{remark}[What this means algorithmically]
If $\beta\|M\|^2<2$, then $E_{\text{GH}}$ is strongly convex and gradient descent converges globally and linearly (Theorem~\ref{thm:rate_strongly_convex}).
Without this condition, the objective can be nonconvex; gradient descent still converges to stationary points
(Theorem~\ref{thm:gd_convergence}), and DC algorithms such as CCCP/MM can be used to obtain monotone descent to critical points
under standard regularity assumptions.
\end{remark}


\subsection{Lipschitz Smoothness}

\begin{prop}[Lipschitz gradient / $L$-smoothness]
\label{prop:lipschitz}
Let
\[
E_{\mathrm{GH}}(X)= -\sum_{v=1}^n \lse(\beta, M x_v)\;+\;\frac12\|X\|_F^2\;+\;\lambda\,\tr(X^\top L X),
\]
with $X\in\mathbb{R}^{n\times d}$, rows $x_v\in\mathbb{R}^d$, memory $M\in\mathbb{R}^{N\times d}$, and Laplacian $L\in\mathbb{R}^{n\times n}$.
Then $\nabla E_{\mathrm{GH}}$ is Lipschitz (in Frobenius norm) with constant
\[
L_{\mathrm{lip}}\;=\;\tfrac{\beta}{2}\|M\|^2\;+\;1\;+\;2\lambda\|L\|,
\]
i.e.
\[
\|\nabla E_{\mathrm{GH}}(X)-\nabla E_{\mathrm{GH}}(Y)\|_F
\;\le\;
L_{\mathrm{lip}}\;\|X-Y\|_F
\qquad \forall X,Y.
\]
\end{prop}

\begin{proof}
Write the gradient in matrix form. Define the row-wise retrieval map
\[
s(x)\;:=\;M^\top \softmax(\beta Mx)\in\mathbb{R}^d,
\qquad
S(X)\in\mathbb{R}^{n\times d}\ \text{by}\ [S(X)]_v:=s(x_v).
\]
From the derivation in the document,
\[
\nabla_X E_{\mathrm{GH}}(X)\;=\;X\;-\;S(X)\;+\;2\lambda L X.
\]
Hence
\begin{align}
\|\nabla E_{\mathrm{GH}}(X)-\nabla E_{\mathrm{GH}}(Y)\|_F
&\le \|X-Y\|_F + \|S(X)-S(Y)\|_F + 2\lambda\|L(X-Y)\|_F \\
&\le \bigl(1+2\lambda\|L\|\bigr)\|X-Y\|_F \;+\;\|S(X)-S(Y)\|_F,
\label{eq:lip_reduce_to_S}
\end{align}
using $\|LZ\|_F\le \|L\|\|Z\|_F$ for any $Z$.

It remains to bound $\|S(X)-S(Y)\|_F$.
Fix a single row $x\in\mathbb{R}^d$ and define $p(x):=\softmax(\beta Mx)\in\mathbb{R}^N$.
The Jacobian of $s(x)=M^\top p(x)$ is
\[
\nabla s(x)
\;=\;
M^\top \nabla p(x)
\;=\;
\beta\,M^\top\Bigl(\diag(p(x)) - p(x)p(x)^\top\Bigr)M.
\]
Let $\Sigma(p):=\diag(p)-pp^\top$. It is positive semidefinite (it is the covariance matrix of a categorical distribution),
and satisfies the tighter bound $\Sigma(p)\preceq \tfrac{1}{2}I$ (since for any unit $u$, $u^\top\Sigma(p)u=\mathrm{Var}(u_I)\le (\max_i u_i-\min_i u_i)^2/4\le 1/2$ by Popoviciu's inequality).
Therefore, for every $x$,
\[
\|\nabla s(x)\|
=
\bigl\|\beta\,M^\top \Sigma(p(x)) M\bigr\|
\le
\beta\,\|M^\top\|\,\|\Sigma(p(x))\|\,\|M\|
\le
\tfrac{\beta}{2}\,\|M\|^2.
\]
By the mean value theorem for vector-valued maps (or integrating the Jacobian along the segment),
this implies $s$ is $\tfrac{\beta}{2}\|M\|^2$-Lipschitz:
\[
\|s(x)-s(y)\|\le \tfrac{\beta}{2}\|M\|^2\|x-y\|\qquad\forall x,y\in\mathbb{R}^d.
\]
Applying this row-wise and summing,
\[
\|S(X)-S(Y)\|_F^2
=
\sum_{v=1}^n \|s(x_v)-s(y_v)\|^2
\le
\sum_{v=1}^n \left(\tfrac{\beta}{2}\|M\|^2\right)^2 \|x_v-y_v\|^2
=
\left(\tfrac{\beta}{2}\|M\|^2\right)^2\|X-Y\|_F^2,
\]
so
\[
\|S(X)-S(Y)\|_F\le \tfrac{\beta}{2}\|M\|^2\|X-Y\|_F.
\]
Plugging this into \eqref{eq:lip_reduce_to_S} gives
\[
\|\nabla E_{\mathrm{GH}}(X)-\nabla E_{\mathrm{GH}}(Y)\|_F
\le
\bigl(\tfrac{\beta}{2}\|M\|^2+1+2\lambda\|L\|\bigr)\|X-Y\|_F,
\]
as claimed. (Note: A looser but still valid bound is $L_{\mathrm{lip}} = \beta\|M\|^2+1+2\lambda\|L\|$ if one uses $\|\Sigma(p)\|\le 1$ instead.)
\end{proof}

\subsection{Convergence Theorem}

\begin{theorem}[Monotone descent and convergence to stationary points]
\label{thm:gd_convergence}
By Proposition~\ref{prop:coercive}, $E_{\mathrm{GH}}$ is coercive and hence bounded below. Let $L_{\mathrm{lip}}$ be as in
Proposition~\ref{prop:lipschitz}.
Consider gradient descent
\[
X^{(t+1)} \;=\; X^{(t)} - \eta \nabla E_{\mathrm{GH}}(X^{(t)}),
\]
with a fixed step size $\eta\in(0,2/L_{\mathrm{lip}})$.
Then:
\begin{enumerate}
\item (\textbf{Energy descent}) The energy is non-increasing, and decreases by at least the stated amount when $\nabla E_{\mathrm{GH}}(X^{(t)})\neq 0$:
\[
E_{\mathrm{GH}}(X^{(t+1)})
\le
E_{\mathrm{GH}}(X^{(t)})
-\eta\Bigl(1-\frac{\eta L_{\mathrm{lip}}}{2}\Bigr)\|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F^2.
\]
\item (\textbf{Gradient norms vanish}) We have
\[
\sum_{t=0}^{\infty}\|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F^2 < \infty,
\qquad\text{hence}\qquad
\|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F \to 0.
\]
\item (\textbf{Limit points are stationary}) Every accumulation point $X^\star$ of $\{X^{(t)}\}$ satisfies
$\nabla E_{\mathrm{GH}}(X^\star)=0$.
\end{enumerate}
\end{theorem}

\begin{proof}
Because $\nabla E_{\mathrm{GH}}$ is $L_{\mathrm{lip}}$-Lipschitz (Proposition~\ref{prop:lipschitz}),
$E_{\mathrm{GH}}$ is $L_{\mathrm{lip}}$-smooth, so the standard descent lemma holds:
for all $X,Y\in\mathbb{R}^{n\times d}$,
\begin{equation}
\label{eq:descent_lemma}
E_{\mathrm{GH}}(Y)
\le
E_{\mathrm{GH}}(X) + \langle \nabla E_{\mathrm{GH}}(X),\,Y-X\rangle
+\frac{L_{\mathrm{lip}}}{2}\|Y-X\|_F^2.
\end{equation}
Apply \eqref{eq:descent_lemma} with $Y=X-\eta\nabla E_{\mathrm{GH}}(X)$:
\begin{align*}
E_{\mathrm{GH}}(X-\eta\nabla E_{\mathrm{GH}}(X))
&\le
E_{\mathrm{GH}}(X)
+\left\langle \nabla E_{\mathrm{GH}}(X),\, -\eta\nabla E_{\mathrm{GH}}(X)\right\rangle
+\frac{L_{\mathrm{lip}}}{2}\eta^2\|\nabla E_{\mathrm{GH}}(X)\|_F^2\\
&=
E_{\mathrm{GH}}(X)
-\eta\Bigl(1-\frac{\eta L_{\mathrm{lip}}}{2}\Bigr)\|\nabla E_{\mathrm{GH}}(X)\|_F^2.
\end{align*}
Setting $X=X^{(t)}$ yields item (1). Since $\eta\in(0,2/L_{\mathrm{lip}})$,
the coefficient $c:=\eta\left(1-\frac{\eta L_{\mathrm{lip}}}{2}\right)$ is strictly positive, hence
$E_{\mathrm{GH}}(X^{(t)})$ is decreasing and bounded below, so it converges.

Summing the inequality from $t=0$ to $T-1$ telescopes:
\[
E_{\mathrm{GH}}(X^{(0)}) - E_{\mathrm{GH}}(X^{(T)})
\ge
c\sum_{t=0}^{T-1}\|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F^2.
\]
Letting $T\to\infty$ and using bounded-below implies the left-hand side is finite, proving
$\sum_t \|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F^2<\infty$, hence $\|\nabla E_{\mathrm{GH}}(X^{(t)})\|_F\to 0$
(item (2)).

For item (3), let $X^{(t_k)}\to X^\star$ be any convergent subsequence.
By continuity of $\nabla E_{\mathrm{GH}}$ (it is Lipschitz),
\[
\nabla E_{\mathrm{GH}}(X^\star)
=
\lim_{k\to\infty} \nabla E_{\mathrm{GH}}(X^{(t_k)})
=
0,
\]
where the last equality uses item (2). Thus $X^\star$ is stationary.
\end{proof}

\subsection{Stability Condition}

For guaranteed convergence with step size $\eta$, using the tighter Lipschitz bound from Proposition~\ref{prop:lipschitz}:

\begin{equation}
\eta < \frac{2}{\tfrac{\beta}{2}\|M\|^2 + 1 + 2\lambda \|L\|}
\label{eq:step_size_bound}
\end{equation}

\textbf{Note:} Throughout this document, we use the unnormalized combinatorial Laplacian $L = D - A$. For this Laplacian with maximum degree $d_{\max}$, we have $\|L\| \le 2d_{\max}$, giving:
\begin{equation}
\eta < \frac{2}{\tfrac{\beta}{2}\|M\|^2 + 1 + 4\lambda d_{\max}}
\label{eq:step_size_bound_unnormalized}
\end{equation}

(If using the looser bound $L_{\text{lip}} = \beta\|M\|^2+1+2\lambda\|L\|$, the step size condition becomes $\eta < 2/(\beta\|M\|^2 + 1 + 2\lambda\|L\|)$.)

\subsection{Alternative: Fixed-Point Convergence}

For the fixed-point iteration in Equation~\eqref{eq:update_fixed}, we need to show it is a contraction mapping.

\begin{remark}[Distinction between GD and fixed-point iteration]
It is important to distinguish two different update schemes:
\begin{enumerate}
\item \textbf{Gradient descent:} $X^{(t+1)} = X^{(t)} - \eta \nabla E_{\text{GH}}(X^{(t)})$ guarantees \emph{energy descent} (Theorem~\ref{thm:gd_convergence}) under smoothness and appropriate step size, regardless of whether the energy is convex.
\item \textbf{Fixed-point iteration:} $X^{(t+1)} = T(X^{(t)})$ (defined below) does \emph{not automatically} imply energy descent. Convergence is guaranteed \emph{only when $T$ is a contraction}, i.e., under the sufficient condition $\tfrac{\beta}{2}\|M\|^2 + 2\lambda\|L\| < 1$ (Theorem~\ref{thm:fp_contraction}).
\end{enumerate}
In practice, gradient descent is more robust, while the fixed-point iteration can be faster when the contraction condition holds.
\end{remark}

\begin{theorem}[Fixed-point contraction for the undamped operator]
\label{thm:fp_contraction}
Define the (row-wise) fixed-point operator $T:\mathbb{R}^{n\times d}\to\mathbb{R}^{n\times d}$ by
\[
[T(X)]_v \;=\; M^\top\softmax(\beta M x_v)\;-\;2\lambda (LX)_v.
\]
Then $T$ is globally Lipschitz (in Frobenius norm) with constant
\[
\mathrm{Lip}(T)\;\le\;\tfrac{\beta}{2}\|M\|^2\;+\;2\lambda\|L\|.
\]
In particular, if the sufficient condition
\[
\tfrac{\beta}{2}\|M\|^2 + 2\lambda\|L\| \;<\; 1
\]
holds, then $T$ is a contraction mapping. Consequently, $T$ has a unique fixed point $X^\star$ and, for any
initialization $X^{(0)}$, the iteration $X^{(t+1)}=T(X^{(t)})$ converges geometrically:
\[
\|X^{(t)}-X^\star\|_F \;\le\; \rho^t \|X^{(0)}-X^\star\|_F,
\qquad
\rho := \tfrac{\beta}{2}\|M\|^2 + 2\lambda\|L\| \in (0,1).
\]
\end{theorem}

\begin{proof}
Decompose $T(X)=S(X)-2\lambda L X$ where, as before, $[S(X)]_v=s(x_v)$ and $s(x)=M^\top\softmax(\beta Mx)$.
From the Jacobian calculation in Proposition~\ref{prop:lipschitz}, $s$ is $\tfrac{\beta}{2}\|M\|^2$-Lipschitz:
\[
\|s(x)-s(y)\|\le \tfrac{\beta}{2}\|M\|^2\|x-y\|\quad\forall x,y.
\]
Therefore, row-wise,
\[
\|S(X)-S(Y)\|_F \le \tfrac{\beta}{2}\|M\|^2\|X-Y\|_F.
\]
Also, linearity of the Laplacian term gives
\[
\|L(X-Y)\|_F \le \|L\|\|X-Y\|_F.
\]
Combining:
\[
\|T(X)-T(Y)\|_F
\le
\|S(X)-S(Y)\|_F + 2\lambda\|L(X-Y)\|_F
\le
\bigl(\tfrac{\beta}{2}\|M\|^2 + 2\lambda\|L\|\bigr)\|X-Y\|_F.
\]
Hence $\mathrm{Lip}(T)\le \tfrac{\beta}{2}\|M\|^2+2\lambda\|L\|$. If this quantity is $<1$,
Banach's fixed-point theorem implies existence and uniqueness of $X^\star$ and geometric convergence with rate $\rho$.
\end{proof}

\begin{corollary}[Convenient sufficient condition for contraction]
For the unnormalized Laplacian $L = D - A$ used throughout this document, we have $\|L\|\le 2d_{\max}$, so a sufficient condition for contraction is
\[
\tfrac{\beta}{2}\|M\|^2 + 4\lambda d_{\max} < 1.
\]
Using the looser bound $\|\Sigma(p)\|\le 1$ would give $\beta\|M\|^2 + 4\lambda d_{\max} < 1$.
\end{corollary}

\section{Connection to Attention Mechanisms}

\subsection{Attention Interpretation}

The Graph Hopfield update can be written as:

\begin{equation}
X \leftarrow \text{Attention}(X, M, M) + \text{GraphSmooth}(X, A)
\end{equation}

where:
\begin{itemize}
    \item $\text{Attention}(Q, K, V) = \softmax(\beta Q K^T) V$ is standard self-attention
    \item $\text{GraphSmooth}(X, A) = -2\lambda L X = -2\lambda (D - A) X$ is graph diffusion/smoothing
\end{itemize}

This makes the Graph Hopfield layer interpretable as: \textbf{"attention-based pattern retrieval + graph-aware smoothing"}.

\subsection{Relation to Transformer Self-Attention}

In transformers, self-attention is:
\begin{equation}
\text{Attention}(X, X, X) = \softmax(\beta X X^T) X
\end{equation}

Graph Hopfield extends this by:
\begin{enumerate}
    \item Using a separate memory bank $M$ instead of $X$ itself
    \item Adding graph structure regularization
\end{enumerate}

\section{Resolved Theoretical Properties}
\label{sec:resolved}

This section answers the remaining theoretical questions: (1) DC structure, (2) explicit convergence rates,
(3) characterization of critical points, and (4) initialization dependence.

\subsection{Convexity / DC Structure}
Proposition~\ref{prop:convex_dc} establishes that $E_{\text{GH}}$ is generally nonconvex (due to a concave
negative log-sum-exp term), admits an explicit \emph{global} sufficient condition for convexity/strong convexity,
and \emph{always} admits a DC decomposition whenever a global smoothness constant is available.

\subsection{Convergence Rates}
We separate the (typical) nonconvex smooth case from regimes where stronger structure yields linear rates.

\begin{theorem}[Sublinear rate to stationarity for smooth (possibly nonconvex) $E_{\text{GH}}$]
\label{thm:rate_nonconvex}
Assume $\nabla E_{\text{GH}}$ is $L_{\text{lip}}$-Lipschitz and $E_{\text{GH}}$ is bounded below by $E_\inf$.
Run gradient descent with fixed step size $\eta\in(0,2/L_{\text{lip}})$:
\[
X^{(t+1)} = X^{(t)} - \eta \nabla E_{\text{GH}}(X^{(t)}).
\]
Let $c := \eta\left(1-\frac{\eta L_{\text{lip}}}{2}\right)>0$. Then for all $T\ge 1$,
\[
\min_{0\le t \le T-1}\ \|\nabla E_{\text{GH}}(X^{(t)})\|_F^2
\le
\frac{E_{\text{GH}}(X^{(0)}) - E_\inf}{c\,T},
\]
and hence the method attains an $\varepsilon$-stationary point
($\|\nabla E\|_F\le \varepsilon$) in at most $O(1/\varepsilon^2)$ iterations.
\end{theorem}

\begin{proof}
From the descent inequality proved in Theorem~\ref{thm:gd_convergence},
\[
E_{\text{GH}}(X^{(t+1)}) \le E_{\text{GH}}(X^{(t)}) - c\|\nabla E_{\text{GH}}(X^{(t)})\|_F^2.
\]
Summing from $t=0$ to $T-1$ and using $E_{\text{GH}}(X^{(T)})\ge E_\inf$ gives
\[
c \sum_{t=0}^{T-1}\|\nabla E_{\text{GH}}(X^{(t)})\|_F^2
\le E_{\text{GH}}(X^{(0)}) - E_\inf.
\]
Divide by $T$ and lower bound the average by the minimum.
\end{proof}

\begin{theorem}[Linear convergence in the strongly convex regime]
\label{thm:rate_strongly_convex}
Assume $E_{\text{GH}}$ is $\mu$-strongly convex and $L_{\text{lip}}$-smooth (e.g., under $\mu=1-\tfrac{\beta\|M\|^2}{2}>0$
from Proposition~\ref{prop:convex_dc}). Let $X^\star$ be the unique minimizer.
With step size $\eta \in (0, 1/L_{\text{lip}}]$, gradient descent satisfies
\[
\|X^{(t)} - X^\star\|_F \;\le\; (1-\eta\mu)^t \|X^{(0)} - X^\star\|_F,
\]
and similarly the function values converge geometrically:
\[
E_{\text{GH}}(X^{(t)}) - E_{\text{GH}}(X^\star)
\;\le\; (1-\eta\mu)^t \bigl(E_{\text{GH}}(X^{(0)}) - E_{\text{GH}}(X^\star)\bigr).
\]
\end{theorem}

\begin{proof}
By strong convexity and smoothness, we have the global Hessian bounds:
\[
\mu I \preceq \nabla^2 E_{\text{GH}}(X) \preceq L_{\text{lip}} I, \qquad \forall X.
\]
For any two points $X, Y$, by the mean value theorem (integral form), there exists a symmetric matrix $A$ (depending on the path from $Y$ to $X$) such that
\[
\nabla E_{\text{GH}}(X) - \nabla E_{\text{GH}}(Y) = A(X - Y),
\]
where $A = \int_0^1 \nabla^2 E_{\text{GH}}(Y + t(X-Y)) \, dt$ satisfies $\mu I \preceq A \preceq L_{\text{lip}} I$.

Consider one gradient descent step:
\[
X^{(t+1)} - X^\star = X^{(t)} - \eta \nabla E_{\text{GH}}(X^{(t)}) - (X^\star - \eta \nabla E_{\text{GH}}(X^\star)).
\]
Since $X^\star$ is a stationary point, $\nabla E_{\text{GH}}(X^\star) = 0$. Applying the mean value theorem with $A^{(t)}$ satisfying $\mu I \preceq A^{(t)} \preceq L_{\text{lip}} I$:
\[
X^{(t+1)} - X^\star = (I - \eta A^{(t)})(X^{(t)} - X^\star).
\]
Taking Frobenius norms:
\[
\|X^{(t+1)} - X^\star\|_F = \|(I - \eta A^{(t)})(X^{(t)} - X^\star)\|_F \le \max_{\lambda \in [\mu, L_{\text{lip}}]} |1 - \eta\lambda| \cdot \|X^{(t)} - X^\star\|_F.
\]
For $\eta \in (0, 1/L_{\text{lip}}]$, the maximum occurs at $\lambda = \mu$, giving:
\[
\|X^{(t+1)} - X^\star\|_F \le (1 - \eta\mu) \|X^{(t)} - X^\star\|_F.
\]
Iterating this inequality yields the geometric convergence in iterates.

For function values, use the descent lemma and strong convexity:
\[
E_{\text{GH}}(X^{(t+1)}) - E_{\text{GH}}(X^\star) \le E_{\text{GH}}(X^{(t)}) - E_{\text{GH}}(X^\star) - \eta \|\nabla E_{\text{GH}}(X^{(t)})\|_F^2 + \frac{L_{\text{lip}}\eta^2}{2}\|\nabla E_{\text{GH}}(X^{(t)})\|_F^2.
\]
By strong convexity, $\|\nabla E_{\text{GH}}(X^{(t)})\|_F^2 \ge 2\mu (E_{\text{GH}}(X^{(t)}) - E_{\text{GH}}(X^\star))$. With $\eta \le 1/L_{\text{lip}}$:
\[
E_{\text{GH}}(X^{(t+1)}) - E_{\text{GH}}(X^\star) \le (1 - \eta\mu)(E_{\text{GH}}(X^{(t)}) - E_{\text{GH}}(X^\star)),
\]
yielding geometric convergence in function values.
\end{proof}

\begin{remark}[Exponential vs linear terminology]
Theorems~\ref{thm:rate_strongly_convex} and the fixed-point contraction theorem both yield
\emph{geometric} rates $(\rho^t)$, often called either \emph{linear convergence} or \emph{exponential decay}.
\end{remark}

\subsection{Local vs Global Minima (Critical Point Characterization)}
We provide what can be said at the level of generality of this writeup.

\begin{prop}[Existence of a global minimizer (coercivity)]
\label{prop:coercive}
Assume $M$ is fixed and finite. Then $E_{\text{GH}}(X)\to +\infty$ as $\|X\|_F\to\infty$.
In particular, $E_{\text{GH}}$ attains at least one global minimizer.
\end{prop}

\begin{proof}
For each node $v$, using the definition $\lse(\beta, \mathbf{z}) = \beta^{-1} \log \sum_i \exp(\beta z_i)$:
\[
\lse(\beta, M x_v) = \beta^{-1}\log\!\sum_{i=1}^N \exp(\beta m_i^\top x_v)
\le \beta^{-1}\left(\log N + \beta \max_i m_i^\top x_v\right)
= \beta^{-1}\log N + \max_i m_i^\top x_v.
\]
Since $\max_i m_i^\top x_v \le \|M\|\,\|x_v\|$ (where $\|M\|$ is the operator/spectral norm, since each row $m_i$ has norm at most $\|M\|$), we have
\[
\lse(\beta, M x_v) \le \beta^{-1}\log N + \|M\|\,\|x_v\|.
\]
Hence
\[
-\lse(\beta, M x_v) \ge -\beta^{-1}\log N - \|M\|\,\|x_v\|.
\]
Summing over $v$ and using $\tr(X^T L X) \ge 0$ (since $L \succeq 0$) gives
\[
E_{\text{GH}}(X) \;\ge\; \frac12\|X\|_F^2 \;-\; \|M\|\sum_v \|x_v\| \;-\; C.
\]
By Cauchy--Schwarz, $\sum_v \|x_v\| \le \sqrt{n}\sqrt{\sum_v \|x_v\|^2} = \sqrt{n}\|X\|_F$, so
\[
E_{\text{GH}}(X) \;\ge\; \frac12\|X\|_F^2 \;-\; \|M\|\sqrt{n}\|X\|_F \;-\; C,
\]
for a constant $C = n\beta^{-1}\log N$ depending only on $(\beta,N,n)$.
The right-hand side diverges to $+\infty$ as $\|X\|_F\to\infty$ (since the quadratic term dominates), so $E_{\text{GH}}$ is coercive.
A coercive continuous function on $\mathbb{R}^{n\times d}$ attains a global minimizer.
\end{proof}

\begin{prop}[Stationary points and second-order characterization]
\label{prop:critical_points}
A point $X^\star$ is a first-order critical point iff $\nabla E_{\text{GH}}(X^\star)=0$
(equivalently, it is a fixed point of the associated fixed-point map).
Moreover:
\begin{itemize}
\item if $\nabla^2 E_{\text{GH}}(X^\star) \succ 0$ then $X^\star$ is a strict local minimizer;
\item if $\nabla^2 E_{\text{GH}}(X^\star)$ has a negative eigenvalue then $X^\star$ is a strict saddle point.
\end{itemize}
If $E_{\text{GH}}$ is convex (e.g. $\beta\|M\|^2\le 2$ from Proposition~\ref{prop:convex_dc}), then \emph{every} critical point is a global minimizer;
if it is strongly convex, that minimizer is unique.
\end{prop}

\begin{proof}
For the first-order characterization: $X^\star$ is a critical point iff $\nabla E_{\text{GH}}(X^\star) = 0$, which is equivalent to being a fixed point of the associated fixed-point map (by Eq.~\eqref{eq:fixed_point}).

For the second-order characterization, use a second-order Taylor expansion around $X^\star$:
\[
E_{\text{GH}}(X^\star + \Delta) = E_{\text{GH}}(X^\star) + \langle \nabla E_{\text{GH}}(X^\star), \Delta \rangle + \frac{1}{2}\langle \Delta, \nabla^2 E_{\text{GH}}(X^\star) \Delta \rangle + o(\|\Delta\|^2).
\]
Since $\nabla E_{\text{GH}}(X^\star) = 0$:
\begin{itemize}
\item If $\nabla^2 E_{\text{GH}}(X^\star) \succ 0$, then for small $\Delta \neq 0$, the quadratic term is positive, so $E_{\text{GH}}(X^\star + \Delta) > E_{\text{GH}}(X^\star)$, making $X^\star$ a strict local minimizer.
\item If $\nabla^2 E_{\text{GH}}(X^\star)$ has a negative eigenvalue with eigenvector $v$, then moving along $v$ (i.e., $X^\star + \varepsilon v$ for small $\varepsilon > 0$) decreases the function, making $X^\star$ a strict saddle point.
\end{itemize}
The convexity statements follow from standard convex optimization theory: in a convex function, all critical points are global minimizers; strong convexity ensures uniqueness.
\end{proof}

\subsection{Initialization Dependence}
Initialization effects depend on whether the landscape is globally well-behaved.

\begin{prop}[When initialization does \emph{not} matter]
\label{prop:init_irrelevant}
If $E_{\text{GH}}$ is strongly convex (e.g. $\beta\|M\|^2<2$ from Proposition~\ref{prop:convex_dc}), then gradient descent converges to the unique
global minimizer $X^\star$ from any initialization (by Theorem~\ref{thm:rate_strongly_convex}).
If the fixed-point map $T$ is a global contraction (Theorem~\ref{thm:fp_contraction}), then fixed-point iteration converges to the unique
fixed point from any initialization (by Banach's fixed-point theorem).
\end{prop}

\begin{remark}[When initialization \emph{does} matter]
In the nonconvex regime, $E_{\text{GH}}$ can have multiple stationary points.
Energy descent guarantees convergence to \emph{some} stationary point, but the selected limit point may depend on:
(i) the starting point $X^{(0)}$ (basins of attraction), and (ii) the step size / algorithmic choices.
Practically, different initializations can lead to different local minima or saddle escapes.

A useful local criterion is stability of a fixed point under the iteration map:
if the Jacobian of the iteration map has spectral radius $<1$ at $X^\star$, then $X^\star$ has a local basin of
attraction (local contraction) and nearby initializations converge to it.
\end{remark}


\section{References}

Key papers to reference:
\begin{itemize}
    \item Universal Hopfield Networks (2202.04557)
    \item Dense Associative Memory for Pattern Recognition (1606.01164)
    \item Hopfield Networks is All You Need (2008.02217)
\end{itemize}

\end{document}
