\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Graph Hopfield Networks: Mathematical Foundation}
\author{Phase 1: Complete Mathematical Derivation}
\date{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator{\lse}{lse}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\softmax}{softmax}

\begin{document}

\maketitle

\section{Introduction}

This document provides the complete mathematical foundation for Graph Hopfield Networks (GHN), extending Modern Continuous Hopfield Networks to graph-structured data. The goal is to derive a rigorous energy function, update dynamics, and prove convergence guarantees.

\section{Background: Modern Continuous Hopfield Networks}

\subsection{Classical Formulation}

For a query vector $\xi \in \mathbb{R}^d$ and a memory bank $M = [m_1, m_2, \ldots, m_N]^T \in \mathbb{R}^{N \times d}$ containing $N$ stored patterns, the Modern Hopfield Network (MCHN) energy function is:

\begin{equation}
E_{\text{MH}}(\xi) = -\beta^{-1} \log \sum_{i=1}^{N} \exp(\beta \cdot \xi^T m_i) + \frac{1}{2}\|\xi\|^2
\label{eq:mhn_energy}
\end{equation}

where $\beta > 0$ is the inverse temperature parameter controlling pattern selectivity.

\subsection{Update Rule}

The fixed-point update rule for MCHN is:

\begin{equation}
\xi_{\text{new}} = M^T \cdot \softmax(\beta \cdot M \xi) = \sum_{i=1}^{N} \frac{\exp(\beta \cdot \xi^T m_i)}{\sum_{j=1}^{N} \exp(\beta \cdot \xi^T m_j)} m_i
\label{eq:mhn_update}
\end{equation}

This can be interpreted as attention-weighted pattern retrieval, where patterns similar to the query receive higher weights.

\subsection{Log-Sum-Exp Function}

The log-sum-exp function is defined as:
\begin{equation}
\lse(\beta, \mathbf{z}) = \beta^{-1} \log \sum_{i=1}^{N} \exp(\beta z_i)
\end{equation}

where $\mathbf{z} = [z_1, \ldots, z_N]^T$. This function is convex and smooth, with gradient:
\begin{equation}
\nabla_{\mathbf{z}} \lse(\beta, \mathbf{z}) = \softmax(\beta \mathbf{z})
\end{equation}

\section{Graph Hopfield Networks: Energy Function}

\subsection{Problem Setup}

Consider a graph $G = (V, E)$ with:
\begin{itemize}
    \item $|V| = n$ nodes
    \item Edge set $E \subseteq V \times V$
    \item Node feature matrix $X = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^{n \times d}$ where $x_v \in \mathbb{R}^d$ is the feature vector for node $v$
    \item Memory bank $M = [m_1, m_2, \ldots, m_N]^T \in \mathbb{R}^{N \times d}$ containing $N$ stored patterns
    \item Adjacency matrix $A \in \{0,1\}^{n \times n}$ where $A_{uv} = 1$ if $(u,v) \in E$
    \item Degree matrix $D = \text{diag}(d_1, \ldots, d_n)$ where $d_v = \sum_{u} A_{uv}$
    \item Graph Laplacian $L = D - A$ (unnormalized) or $L = I - D^{-1/2} A D^{-1/2}$ (normalized)
\end{itemize}

\subsection{Graph Hopfield Energy Function}

The Graph Hopfield energy function combines associative memory retrieval with graph structure regularization:

\begin{equation}
E_{\text{GH}}(X) = \sum_{v \in V} \left[ -\beta^{-1} \lse(\beta, M^T x_v) + \frac{1}{2}\|x_v\|^2 \right] + \lambda \sum_{(u,v) \in E} \|x_u - x_v\|^2
\label{eq:gh_energy_node}
\end{equation}

where:
\begin{itemize}
    \item First term: Hopfield retrieval energy per node (attracts node embeddings toward stored patterns)
    \item Second term: Graph Laplacian regularization (enforces smoothness between neighboring nodes)
    \item $\beta > 0$: Inverse temperature (controls pattern selectivity)
    \item $\lambda \geq 0$: Graph smoothness weight
\end{itemize}

\subsection{Matrix Formulation}

The energy can be written in compact matrix form:

\begin{equation}
E_{\text{GH}}(X) = -\beta^{-1} \sum_{v=1}^{n} \lse(\beta, M^T x_v) + \frac{1}{2}\|X\|_F^2 + \lambda \cdot \tr(X^T L X)
\label{eq:gh_energy_matrix}
\end{equation}

where $\|X\|_F^2 = \sum_{v=1}^{n} \|x_v\|^2$ is the Frobenius norm squared, and $L$ is the graph Laplacian.

\textbf{Derivation of Laplacian term:}
\begin{align}
\sum_{(u,v) \in E} \|x_u - x_v\|^2 &= \sum_{(u,v) \in E} (x_u - x_v)^T (x_u - x_v) \\
&= \sum_{(u,v) \in E} (x_u^T x_u - 2 x_u^T x_v + x_v^T x_v) \\
&= \sum_{v} d_v x_v^T x_v - 2 \sum_{(u,v) \in E} x_u^T x_v \\
&= \tr(X^T D X) - \tr(X^T A X) \\
&= \tr(X^T (D - A) X) = \tr(X^T L X)
\end{align}

\section{Update Dynamics}

\subsection{Gradient Computation}

To derive the update dynamics, we compute the gradient of the energy with respect to each node's feature vector.

\textbf{For node $v$:}

\begin{align}
\nabla_{x_v} E_{\text{GH}}(X) &= \nabla_{x_v} \left[ -\beta^{-1} \lse(\beta, M^T x_v) + \frac{1}{2}\|x_v\|^2 + \lambda \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2 \right] \\
&= -\beta^{-1} \nabla_{x_v} \lse(\beta, M^T x_v) + x_v + \lambda \nabla_{x_v} \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2
\end{align}

\textbf{First term (Hopfield retrieval):}
\begin{align}
\nabla_{x_v} \lse(\beta, M^T x_v) &= \nabla_{x_v} \left[ \beta^{-1} \log \sum_{i=1}^{N} \exp(\beta \cdot m_i^T x_v) \right] \\
&= \frac{\sum_{i=1}^{N} \exp(\beta \cdot m_i^T x_v) \cdot \beta m_i}{\sum_{j=1}^{N} \exp(\beta \cdot m_j^T x_v)} \\
&= \sum_{i=1}^{N} \frac{\exp(\beta \cdot m_i^T x_v)}{\sum_{j=1}^{N} \exp(\beta \cdot m_j^T x_v)} m_i \\
&= M^T \softmax(\beta \cdot M x_v)
\end{align}

\textbf{Third term (Graph regularization):}
\begin{align}
\nabla_{x_v} \sum_{u \in \mathcal{N}(v)} \|x_v - x_u\|^2 &= \sum_{u \in \mathcal{N}(v)} 2(x_v - x_u) \\
&= 2d_v x_v - 2 \sum_{u \in \mathcal{N}(v)} x_u \\
&= 2(L X)_v
\end{align}

where $(L X)_v$ denotes the $v$-th row of $L X$.

\textbf{Complete gradient:}
\begin{equation}
\nabla_{x_v} E_{\text{GH}}(X) = -M^T \softmax(\beta \cdot M x_v) + x_v + 2\lambda (L X)_v
\label{eq:gradient}
\end{equation}

\subsection{Fixed-Point Iteration}

Using gradient descent with step size $\eta > 0$:

\begin{equation}
x_v^{(t+1)} = x_v^{(t)} - \eta \nabla_{x_v} E_{\text{GH}}(X^{(t)})
\end{equation}

Substituting the gradient:

\begin{equation}
x_v^{(t+1)} = x_v^{(t)} + \eta \left[ M^T \softmax(\beta \cdot M x_v^{(t)}) - x_v^{(t)} - 2\lambda (L X^{(t)})_v \right]
\label{eq:update_gd}
\end{equation}

\subsection{Alternative: Direct Fixed-Point Update}

Setting the gradient to zero gives the fixed-point condition:

\begin{equation}
x_v^* = M^T \softmax(\beta \cdot M x_v^*) - 2\lambda (L X^*)_v
\label{eq:fixed_point}
\end{equation}

This suggests an iterative update rule:

\begin{equation}
x_v^{(t+1)} = M^T \softmax(\beta \cdot M x_v^{(t)}) - 2\lambda (L X^{(t)})_v
\label{eq:update_fixed}
\end{equation}

\subsection{With Damping/Momentum}

For numerical stability, we can add damping:

\begin{equation}
x_v^{(t+1)} = (1-\alpha) x_v^{(t)} + \alpha \left[ M^T \softmax(\beta \cdot M x_v^{(t)}) - 2\lambda (L X^{(t)})_v \right]
\label{eq:update_damped}
\end{equation}

where $\alpha \in (0,1]$ is the damping coefficient.

\section{Convergence Analysis}

\subsection{Convexity Properties}

\begin{lemma}[Convexity of Log-Sum-Exp]
The function $f(x) = \lse(\beta, M^T x)$ is convex in $x$.
\end{lemma}

\begin{proof}
The log-sum-exp function is the composition of:
\begin{itemize}
    \item Linear function: $g(x) = M^T x$ (convex)
    \item Log-sum-exp: $h(z) = \beta^{-1} \log \sum_i \exp(\beta z_i)$ (convex)
\end{itemize}
Since the composition of a convex function with an affine function is convex, $f(x)$ is convex.
\end{proof}

\begin{lemma}[Convexity of Quadratic Regularization]
The function $f(X) = \frac{1}{2}\|X\|_F^2$ is strictly convex.
\end{lemma}

\begin{proof}
The Hessian is $\nabla^2 f(X) = I$, which is positive definite. Therefore, $f$ is strictly convex.
\end{proof}

\begin{lemma}[Convexity of Laplacian Term]
The function $f(X) = \tr(X^T L X)$ is convex (but not strictly convex if $L$ is singular).
\end{lemma}

\begin{proof}
The graph Laplacian $L$ is positive semi-definite (all eigenvalues $\geq 0$). Therefore, for any $X, Y$ and $\theta \in [0,1]$:
\begin{align}
\tr((\theta X + (1-\theta)Y)^T L (\theta X + (1-\theta)Y)) &= \theta^2 \tr(X^T L X) + (1-\theta)^2 \tr(Y^T L Y) + 2\theta(1-\theta)\tr(X^T L Y) \\
&\leq \theta \tr(X^T L X) + (1-\theta) \tr(Y^T L Y)
\end{align}
where the inequality follows from the positive semi-definiteness of $L$. Therefore, $f$ is convex.
\end{proof}

\begin{proposition}[Convexity of Graph Hopfield Energy]
The Graph Hopfield energy function $E_{\text{GH}}(X)$ is convex in $X$.
\end{proposition}

\begin{proof}
$E_{\text{GH}}(X)$ is the sum of:
\begin{enumerate}
    \item $-\beta^{-1} \sum_v \lse(\beta, M^T x_v)$: negative of convex function (concave), but...
    \item $\frac{1}{2}\|X\|_F^2$: strictly convex
    \item $\lambda \tr(X^T L X)$: convex
\end{enumerate}

\textbf{Note:} The first term is actually concave (negative log-sum-exp), so the overall energy is NOT necessarily convex. However, if the quadratic regularization term dominates (which it does for large enough $\|X\|$), the energy function may still have good convergence properties.

\textbf{Alternative approach:} We need to analyze the energy more carefully. The negative log-sum-exp term is concave, so the energy is a difference of convex (DC) function. This suggests using the Concave-Convex Procedure (CCCP) or analyzing convergence under specific conditions.
\end{proof}

\subsection{Lipschitz Smoothness}

\begin{proposition}[Lipschitz Gradient]
The gradient $\nabla_X E_{\text{GH}}(X)$ is Lipschitz continuous with Lipschitz constant:
\begin{equation}
L_{\text{lip}} = \beta \|M\|^2 + 1 + 2\lambda \|L\|
\end{equation}
where $\|M\|$ is the spectral norm of $M$ and $\|L\|$ is the spectral norm of $L$.
\end{proposition}

\begin{proof}
\textbf{To be completed:} Need to bound the Hessian of $E_{\text{GH}}(X)$.

The Hessian has three components:
\begin{enumerate}
    \item Hessian of $-\beta^{-1} \lse(\beta, M^T x_v)$: Need to compute second derivative
    \item Hessian of $\frac{1}{2}\|x_v\|^2$: Identity matrix
    \item Hessian of $\lambda \tr(X^T L X)$: $2\lambda L$
\end{enumerate}

For the first term, the Hessian with respect to $x_v$ is:
\begin{equation}
\nabla_{x_v}^2 \lse(\beta, M^T x_v) = \beta M^T \left[ \text{diag}(\softmax(\beta M x_v)) - \softmax(\beta M x_v) \softmax(\beta M x_v)^T \right] M
\end{equation}

This is bounded by $\beta \|M\|^2$ (since the matrix in brackets has eigenvalues in $[0,1]$).

Therefore, the overall Lipschitz constant is:
\begin{equation}
L_{\text{lip}} \leq \beta \|M\|^2 + 1 + 2\lambda \|L\|
\end{equation}
\end{proof}

\subsection{Convergence Theorem}

\begin{theorem}[Convergence of Gradient Descent]
For the Graph Hopfield energy $E_{\text{GH}}(X)$, gradient descent with step size $\eta < \frac{2}{L_{\text{lip}}}$ converges to a stationary point, where $L_{\text{lip}} = \beta \|M\|^2 + 1 + 2\lambda \|L\|$.
\end{theorem}

\begin{proof}
\textbf{To be completed:} This requires:
\begin{enumerate}
    \item Showing the energy decreases monotonically: $E(X^{(t+1)}) \leq E(X^{(t)})$
    \item Bounding the energy decrease per iteration
    \item Showing that gradient norms converge to zero
    \item Establishing convergence rate (linear/sublinear)
\end{enumerate}

\textbf{Key steps:}
\begin{enumerate}
    \item Use Lipschitz smoothness: For $L$-smooth function, gradient descent with $\eta < 2/L$ decreases energy
    \item Energy decrease: $E(X^{(t+1)}) \leq E(X^{(t)}) - \eta(1 - \frac{\eta L}{2}) \|\nabla E(X^{(t)})\|^2$
    \item Sum over iterations to show gradient converges to zero
    \item Use convexity (or weaker conditions) to show convergence to global/local minimum
\end{enumerate}
\end{proof}

\subsection{Stability Condition}

For guaranteed convergence with step size $\eta$:

\begin{equation}
\eta < \frac{2}{\beta \|M\|^2 + 1 + 2\lambda \|L\|}
\label{eq:step_size_bound}
\end{equation}

If using normalized Laplacian and assuming maximum degree $d_{\max}$:

\begin{equation}
\eta < \frac{2}{\beta \|M\|^2 + 1 + 2\lambda \cdot d_{\max}}
\label{eq:step_size_bound_degree}
\end{equation}

\subsection{Alternative: Fixed-Point Convergence}

For the fixed-point iteration in Equation~\eqref{eq:update_fixed}, we need to show it is a contraction mapping.

\begin{theorem}[Fixed-Point Convergence]
If the fixed-point update operator $T(X)$ defined by:
\begin{equation}
T(X)_v = M^T \softmax(\beta \cdot M x_v) - 2\lambda (L X)_v
\end{equation}
is a contraction mapping, then the iteration converges to a unique fixed point.
\end{theorem}

\begin{proof}
\textbf{To be completed:} Need to show:
\begin{equation}
\|T(X) - T(Y)\| \leq \rho \|X - Y\|
\end{equation}
for some $\rho < 1$.

This requires bounding:
\begin{enumerate}
    \item The Lipschitz constant of the softmax function
    \item The effect of the Laplacian term
    \item Combining both to show overall contraction
\end{enumerate}
\end{proof}

\section{Connection to Attention Mechanisms}

\subsection{Attention Interpretation}

The Graph Hopfield update can be written as:

\begin{equation}
X \leftarrow \text{Attention}(X, M, M) + \text{GraphSmooth}(X, A)
\end{equation}

where:
\begin{itemize}
    \item $\text{Attention}(Q, K, V) = V \cdot \softmax(\beta Q K^T)$ is standard self-attention
    \item $\text{GraphSmooth}(X, A) = -\lambda L X = -\lambda (D - A) X$ is graph diffusion/smoothing
\end{itemize}

This makes the Graph Hopfield layer interpretable as: \textbf{"attention-based pattern retrieval + graph-aware smoothing"}.

\subsection{Relation to Transformer Self-Attention}

In transformers, self-attention is:
\begin{equation}
\text{Attention}(X, X, X) = X \cdot \softmax(\beta X X^T)
\end{equation}

Graph Hopfield extends this by:
\begin{enumerate}
    \item Using a separate memory bank $M$ instead of $X$ itself
    \item Adding graph structure regularization
\end{enumerate}

\section{Open Questions for Convergence Proof}

The following need to be rigorously proven:

\begin{enumerate}
    \item \textbf{Convexity/DC Structure:} Is $E_{\text{GH}}(X)$ convex, or is it a difference-of-convex (DC) function? If DC, what convergence guarantees can we establish?
    
    \item \textbf{Lipschitz Constant:} Compute the exact Lipschitz constant of $\nabla_X E_{\text{GH}}(X)$ and verify the bound in Proposition 2.
    
    \item \textbf{Energy Decrease:} Prove that gradient descent (or fixed-point iteration) monotonically decreases the energy: $E(X^{(t+1)}) \leq E(X^{(t)})$.
    
    \item \textbf{Gradient Convergence:} Show that $\lim_{t \to \infty} \|\nabla E(X^{(t)})\| = 0$.
    
    \item \textbf{Convergence Rate:} Establish whether convergence is linear, sublinear, or exponential, and provide explicit rates.
    
    \item \textbf{Fixed-Point Contraction:} For the fixed-point iteration, prove it is a contraction mapping and find the contraction factor.
    
    \item \textbf{Local vs Global Minima:} Characterize the nature of critical points (local minima, saddle points, global minima).
    
    \item \textbf{Initialization Dependence:} Analyze how initialization affects convergence and final solution.
\end{enumerate}

\section{References}

Key papers to reference:
\begin{itemize}
    \item Universal Hopfield Networks (2202.04557)
    \item Dense Associative Memory for Pattern Recognition (1606.01164)
    \item Hopfield Networks is All You Need (2008.02217)
\end{itemize}

\end{document}
